---
title: "Computer Fundamentals: History and Fundamentals"
date: "07-06-2023"
author: "Andrei Guevorkian"
---
Is your smartphone considered a computer? How about the components inside your oven that allow you to connect to it via an app? What are the minimum requirements for something to be considered a computer? That's what we'll explore in this article.

# Table of Contents

[A Brief History of Computing](#a-brief-history-of-computing)
- [The Jacaquard Loom: Breakthrough in Automation](#the-jacaquard-loom-breakthrough-in-automation)
- [Charles Babbage and the Analytical Engine: Conceptualizing the Computer](#charles-babbage-and-the-analytical-engine-conceptualizing-the-computer)
- [Electronic Computing: WWII's Technological Impact](#electronic-computing-wwiis-technological-impact)
- [Computer Terminals: An Advancement in Human-Computer Interaction](#computer-terminals-an-advancement-in-human-computer-interaction) 

[The Modern State of Computers](#the-modern-state-of-computers)

[Computer Fundamentals](#computer-fundamentals)


# A Brief History of Computing

## The Jacaquard Loom: Breakthrough in Automation

Before the year 1805, if I wanted to make a piece of cloth, such as a napkin, with a beautiful design on it, I would have to use a device called a handloom and manually control the position of each thread to create the desired pattern. And if I wanted to make two such napkins, I'd have to redo the entire process.

However, then came the Jacquard loom, named after the French weaver Joseph-Marie Jacquard. This device used a system of stiff paper with holes punched into them in specific patterns, which was a representation of a portion of the actual design pattern. So now, instead of manually controlling the threads with my hands, I would insert a series of these punch cards into the loom, and the machine would lift and lower threads based on the hole positions on the card. 

> Watch [this short animation](https://www.youtube.com/watch?v=K6NgMNvK52A) demonstrating this process. (**render on page directly maybe**)

What does this have to do with computers? Well, while the Jacquard loom is not directly related to the development of computers, it started a conversation about input and output, as well as automation and "feeding instructions" to a machine. 

"Input" is what goes "in" a system, and "output" is what comes "out" as a result. Before the Jacquard loom, direct human intervention was critical in the input to get the desired output. In the case of the Jacquard loom, at a given moment, the input is a punch card, and the output is a row of threads being woven in a specific pattern, all without the "human touch".

This invention laid the foundation for key concepts in computing, notably the idea of feeding punch cards (containing instructions) as input to a machine. It foreshadowed the later development of computing devices.

## Charles Babbage and the Analytical Engine: Conceptualizing the Computer

The first device considered a computer was the Analytical Engine, conceptualized by English inventor Charles Babbage in the 1830s. Although it was never fully constructed during his lifetime, Babbage's designs and ideas laid the foundation for modern computing.

What made this device the first computer? Well, because it had all functionalities of one: there was a reader to accept inputs in the form of punch cards, a printer to output and print the results, a processing unit to perform computations, and a storage unit to store, retrieve, and manipulate data. The Analytical Engine was an early, mechanical version of a general-purpose computer capable of solving various math problems using a steam engine to power the machine and turn all its gears.

> To get an idea of what the Analytical Engine would've looked liked, [watch this video](https://www.youtube.com/watch?v=be1EM3gQkAY).

## Electronic Computing: WWII's Technological Impact

It wasn't until a hundred years after Babbage conceptualized his Analytical Engine that the first physical computer was actually built. We are now in the early 1940s, in the middle of the Second World War, and electricity has played a major role in the development of many new technologies, replacing its mechanical counteparts.

During this time, several pioneering computers were independently designed in Germany (the Z3 in 1941), the United States (the ABC in 1942), and the United Kingdom (the Colussus in 1943), representing significant advancements in the field of electronic computing. The large investment into this field was fueled by the desire to develop secure communication and encryption systems, decrypt the enemy's military communications, as well as predict weapon trajectory and analyze data in general. At the root of all of these problems is mathematics, hence why computers were important.

As it turned out, the binary representation of numbers was easy to work with in order to perform all the calculations.

> To learn more about binary numbers, [check this out](https://www.mathsisfun.com/binary-number-system.html)!

This was not only perfect with the use of punch cards and punch tapes, where the presence of a hole can be represented with a 0, and the absence of a hole can be represented by a 1, but it also worked out well with the next technological advancement: vacuum tubes. They look like little light bulbs, and in some sense they were, but more importantly, they were electronic devices that controlled and manipulated electrical signals, providing the means to process and control data electronically, which in turn enabled their use in mathematical calculations. If the light is on, that's a '1' in binary, and if there is no light, that's a '0'. That's the fundamental idea.

> [Take a look at the ENIAC](https://www.youtube.com/watch?v=HgsklKafxG8): the world's first fully functional, general-purpose electronic computer.

## Computer Terminals: An Advancement in Human-Computer Interaction

What we have so far are tens of thousands of vacuum tubes and just as many small electronic components, making up a room-sized computer, known as the mainframe. As the mainframes themselves evolved, there arose a need for better ways for humans to interact with them. 

The input/output, or I/O, conversation began in the early 1800s with the Jacquard loom, and has evolved from punch cards, to punch tapes, to teletype machines, and finally, in the 1960s, the terminal which uses a CRT display, or cathode-ray tubes.

CRT displays transmitted light onto a screen, and this meant that it could be written and re-written endlessly, as opposed to paper copies. With CRTs, we now had a device, called a terminal, which consisted of a screen and a keyboard, enabling users to input text-based instructions using the keyboard, and receive output from the mainframe computer by looking at the screen. This terminal served as an interface between the user and the mainframe computer. 

The terminal screen consisted of a black background, with green or white alphanumeric characters. One reason for the black background was to reduce eye strain; a white background would cause eye fatigue and glare, and it would also require a significant amount of power.

The invention of computer terminals significantly improved the efficiency and usability of computers. It enabled users to interact with the machines in a more intuitive and direct manner, enabling the growth and adoption of computing technology.

# The Modern State of Computers

Today, we no longer have room-sized mainframes made of vacuum tubes. Nor do we communicate with our computers using text-based commands through a device called a terminal... well actually, some of us still sort of do that. I'll explain later. But it is true that the average person will never need to encounter anything that resembles a terminal. Large corporations such as Microsoft and Apple make their systems very easy to use, so much so that our grandparents can also catch on with the latest tech. Onn top of a keyboard, we mainly use a mouse to use visual applications like Photoshop, watch videos on Youtube, save pictures on our computers, and scroll down social media. Imagine having to write a text command to open your web browser. Imagine going on your social media account, and seeing only text.

That's mainly because the modern state of computers is characterized by user-friendly interfaces, notably graphical user interfaces (GUIs). Instead of typing complex commands in a terminal, we interact with our computers using intuitive visual elements such as icons, buttons, menus, and windows.

We moved away from vacuum tubes, and now have tiny transistors, billions of them, all tightly packed inside a laptop, which can sit on our lap. We no longer use the terms "mainframe"; the mainframe is now simply the computer or laptop, simply reduced in size considerably. And as for the terminal, the device that used to be separate from the mainframe and used to communicate with it, that is just the screen and the laptop's keyboard itself.

Transistors allowed for the miniaturization of electronic components, paving the way for the development of integrated circuits (ICs) in the late 1950s and early 1960s. ICs combined multiple transistors, resistors, and capacitors onto a single silicon chip, enabling even greater reductions in size and power consumption. These ICs are used extensively in various electronic devices, ranging from smartphones and laptops to IoT (Internet of Things) devices, wearable technology, home automation systems, and smart appliances such as thermostats, refrigerators, and voice-controlled assistants.

# Computer Fundamentals

Watching cat videos on a computer is cool, but what is a computer fundamentally? What makes a given device a "computer"? Because clearly, if our laptops didn't give us access to cat videos, it would still be considered a computer... right?

Well, the word "computer" comes from the verb "to compute", and by adding "er" to the end it means "someone that computes", just like what "writer" is to "write", "dancer" is to "dance", and "programmer" is to "program".

Since the first use of the word in the 1600's, "computers" have evolved. They have gone from a person who computes, to machines that compute. But is computing all that is really required nowadays to be considered a "computer"? For example, take [a "computer" made up of dominoes](https://www.youtube.com/watch?v=w6E7aQnA4Ws). Can this system technically be considered a computer? Even though they call it a computer in the video, they are using the looser definition of "computer" as "something that computes". Following a slightly stricter definition of a modern computer, the answer would be no.

Why isn't a ["domino computer"](https://www.youtube.com/watch?v=OpLU__bhu2w) or a ["water computer"](https://www.youtube.com/watch?v=IxXaizglscw) technically a computer? Because in order to be a computer, you have to do more than just perform mathematical computations. That's why a basic calculator isn't a computer.

For one, these systems don't have a way to store data or retrieve previous calculations. They don't provide the possibility to store any data.

Think back to Charles Babbage's Analytical Engine. What were its components? A device that takes in punch cards (the input), a device that performs calculations on the inputs (the processing unit), a device that can store numbers (the memory), and a way to show the user the result of the calculations (the output).

Now what about those calculators that can store and display some of the last few calculation results. Or what if we could store numbers using dominoes. Would these be considered computers?

Still not. The last key ingredient for a system or device to be a computer is to be programmable. What does it mean to be programmable?


Itâ€™s programmable because you can upload your own code into it and have it do whatever task you want.



doesnt need to have keyboard monitor

provide link to turing completeness if interested. add a sentence

==========================================================================================

1. why wasn't the jacquard loom considered a computer? what made the babbage analytical engine be a "computer"?
2. what about a hypothetical device that just takes in sound, and outputs sound. would that be labeled as a computer? why or why not?
3. what are the basic requirements for a device to be considered a computer? what is the bare minimum. what is the simplest computer a person can make?
4. analyze situation of embedded device in fridge. 
5. talk about the computational complexity spectrum, what are the extremes (maybe). talk about software and hardware, talk how can connect external devices using drivers. can get complicated. 
important: make sure to explain what is a computer and what it is not. don't just give boring definitions.

6. mention a sentence or two on: we can simulate a computer using a computer

Add some tips,fun facts, notes and questions along the way

> WHAT DO YOU THINK IS A COMPUTER? WHAT IS NOT A COMPUTER?

> Name a computer around you that usually people don't see as a computer.

> Take a device that accepts only sound as input. The output will be the same sound, but one octave higher

DISPERSE THESE THROUGHOUT

Mention about the modern OS:
Now talk about how to organize everything, drivers, etc. you want a special software that will manage all of those things that we don't need the user to manage. 


From punch cards, we moved on to keyboards, mice, and monitors as primary I/O devices when personal computers began to be common in the late 20th century. Keyboards and mice provide input, allowing us to tell the computer what to do, while monitors provide output, letting us see what the computer is doing or has done.



do we have a definitive guide to knowing if a device is a "computer"? tell me the most basic computer one could build, going all the way up to the most complicated, and name stuff in between
ChatGPT
Certainly! Here's a spectrum of computing devices, ranging from the most basic to the most complex:

Basic Computer: At the simplest end of the spectrum, a basic computer can be built using fundamental components, such as a central processing unit (CPU), memory, and input/output (I/O) devices. This could involve using a microcontroller or a single-board computer (SBC) like the Raspberry Pi. It would typically have limited computational capabilities and storage capacity, and may require programming to perform specific functions.

Embedded Systems: Embedded systems are specialized computers designed for specific applications or tasks. These devices often have dedicated functions and are tightly integrated into other systems. Examples include smart appliances, industrial control systems, and automotive electronics. They typically have limited resources, specific input/output interfaces, and may run custom firmware or software.

Personal Computers (PCs): Personal computers are general-purpose computers designed for individual use. They consist of a CPU, memory, storage, I/O devices (such as a keyboard, mouse, and monitor), and an operating system. PCs offer a broad range of applications and are highly programmable, allowing users to install software and perform diverse tasks.

Workstations and Servers: Workstations and servers are more powerful computing systems, typically used for complex tasks and data processing. They have higher processing capabilities, larger memory capacities, and can handle demanding workloads. Workstations are used for tasks like 3D rendering, scientific simulations, and computer-aided design, while servers are designed to manage and distribute resources across networks.

Mainframe Computers: Mainframes are large-scale computers designed for handling extensive computational tasks and managing critical data. They have massive processing power, extensive memory, and advanced capabilities for high-volume data processing, transaction processing, and concurrent user access. Mainframes are used in industries where reliability, security, and scalability are crucial, such as banking, healthcare, and government.

Supercomputers: Supercomputers are the most powerful computers available. They are used for complex scientific simulations, data modeling, and high-performance computing. Supercomputers have massive processing power, parallel architectures, and specialized designs to solve problems requiring extensive computational resources.

It's important to note that the boundaries between these categories can sometimes blur, as technology advances and computing devices become more specialized or versatile. The classification of a device as a "computer" ultimately depends on its ability to process data, execute instructions, and provide programmability to varying degrees.
